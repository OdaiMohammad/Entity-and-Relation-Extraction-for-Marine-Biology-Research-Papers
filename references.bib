@article{Goyal2018RNE,
author = {Goyal, Archana and Gupta, Vishal and Kumar, Manish},
year = {2018},
month = {08},
pages = {21-43},
title = {Recent Named Entity Recognition and Classification techniques: A systematic review},
volume = {29},
journal = {Computer Science Review},
doi = {10.1016/j.cosrev.2018.06.001}
}

@misc{ACE,
  title = {ACE},
  author={Linguistic Data Consortium, The Trustees of the University of Pennsylvania},
  howpublished = {\url{https://www.ldc.upenn.edu/collaborations/past-projects/ace}}
}


@misc{TAC,
  title = {TAC Relation Extraction Dataset LDC2018T24},
  author={Zhong, Victor, et al.}, 
year = {2018},
month = {12},
doi = {https://doi.org/10.35111/m0kp-4w25},
  howpublished = {\url{https://catalog.ldc.upenn.edu/LDC2018T24}}
}

@misc{NYTRE,
  title = {New York Times Relation Extraction Dataset},
  author={Shantanu Tripathi},
  howpublished = {\url{https://www.kaggle.com/datasets/daishinkan002/new-york-times-relation-extraction-dataset/data}}
}

@InProceedings{luan2018multitask,
     author = {Luan, Yi and He, Luheng and Ostendorf, Mari and Hajishirzi, Hannaneh},
     title = {Multi-Task Identification of Entities, Relations, and Coreferencefor Scientific Knowledge Graph Construction},
     booktitle = {Proc.\ Conf. Empirical Methods Natural Language Process. (EMNLP)},
     year = {2018},
}

@inproceedings{li-ji-2014-incremental,
    title = "Incremental Joint Extraction of Entity Mentions and Relations",
    author = "Li, Qi  and
      Ji, Heng",
    editor = "Toutanova, Kristina  and
      Wu, Hua",
    booktitle = "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jun,
    year = "2014",
    address = "Baltimore, Maryland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P14-1038",
    doi = "10.3115/v1/P14-1038",
    pages = "402--412",
}

@inproceedings{zhang-etal-2017-end,
    title = "End-to-End Neural Relation Extraction with Global Optimization",
    author = "Zhang, Meishan  and
      Zhang, Yue  and
      Fu, Guohong",
    editor = "Palmer, Martha  and
      Hwa, Rebecca  and
      Riedel, Sebastian",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1182",
    doi = "10.18653/v1/D17-1182",
    pages = "1730--1740",
    abstract = "Neural networks have shown promising results for relation extraction. State-of-the-art models cast the task as an end-to-end problem, solved incrementally using a local classifier. Yet previous work using statistical models have demonstrated that global optimization can achieve better performances compared to local classification. We build a globally optimized neural model for end-to-end relation extraction, proposing novel LSTM features in order to better learn context representations. In addition, we present a novel method to integrate syntactic information to facilitate global learning, yet requiring little background on syntactic grammars thus being easy to extend. Experimental results show that our proposed model is highly effective, achieving the best performances on two standard benchmarks.",
}

@inproceedings{wang-lu-2020-two,
    title = "Two are Better than One: Joint Entity and Relation Extraction with Table-Sequence Encoders",
    author = "Wang, Jue  and
      Lu, Wei",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.133",
    doi = "10.18653/v1/2020.emnlp-main.133",
    pages = "1706--1721",
    abstract = "Named entity recognition and relation extraction are two important fundamental problems. Joint learning algorithms have been proposed to solve both tasks simultaneously, and many of them cast the joint task as a table-filling problem. However, they typically focused on learning a single encoder (usually learning representation in the form of a table) to capture information required for both tasks within the same space. We argue that it can be beneficial to design two distinct encoders to capture such two different types of information in the learning process. In this work, we propose the novel table-sequence encoders where two different encoders {--} a table encoder and a sequence encoder are designed to help each other in the representation learning process. Our experiments confirm the advantages of having two encoders over one encoder. On several standard datasets, our model shows significant improvements over existing approaches.",
}

@inproceedings{miwa-sasaki-2014-modeling,
    title = "Modeling Joint Entity and Relation Extraction with Table Representation",
    author = "Miwa, Makoto  and
      Sasaki, Yutaka",
    editor = "Moschitti, Alessandro  and
      Pang, Bo  and
      Daelemans, Walter",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1200",
    doi = "10.3115/v1/D14-1200",
    pages = "1858--1869",
}

@inproceedings{katiyar-cardie-2017-going,
    title = "Going out on a limb: Joint Extraction of Entity Mentions and Relations without Dependency Trees",
    author = "Katiyar, Arzoo  and
      Cardie, Claire",
    editor = "Barzilay, Regina  and
      Kan, Min-Yen",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1085",
    doi = "10.18653/v1/P17-1085",
    pages = "917--928",
    abstract = "We present a novel attention-based recurrent neural network for joint extraction of entity mentions and relations. We show that attention along with long short term memory (LSTM) network can extract semantic relations between entity mentions without having access to dependency trees. Experiments on Automatic Content Extraction (ACE) corpora show that our model significantly outperforms feature-based joint model by Li and Ji (2014). We also compare our model with an end-to-end tree-based LSTM model (SPTree) by Miwa and Bansal (2016) and show that our model performs within 1{\%} on entity mentions and 2{\%} on relations. Our fine-grained analysis also shows that our model performs significantly better on Agent-Artifact relations, while SPTree performs better on Physical and Part-Whole relations.",
}

@inproceedings{zheng-etal-2017-joint,
    title = "Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme",
    author = "Zheng, Suncong  and
      Wang, Feng  and
      Bao, Hongyun  and
      Hao, Yuexing  and
      Zhou, Peng  and
      Xu, Bo",
    editor = "Barzilay, Regina  and
      Kan, Min-Yen",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1113",
    doi = "10.18653/v1/P17-1113",
    pages = "1227--1236",
    abstract = "Joint extraction of entities and relations is an important task in information extraction. To tackle this problem, we firstly propose a novel tagging scheme that can convert the joint extraction task to a tagging problem.. Then, based on our tagging scheme, we study different end-to-end models to extract entities and their relations directly, without identifying entities and relations separately. We conduct experiments on a public dataset produced by distant supervision method and the experimental results show that the tagging based methods are better than most of the existing pipelined and joint learning methods. What{'}s more, the end-to-end model proposed in this paper, achieves the best results on the public dataset.",
}

@article{Zhong2020AFE,
  title={A Frustratingly Easy Approach for Joint Entity and Relation Extraction},
  author={Zexuan Zhong and Danqi Chen},
  journal={ArXiv},
  year={2020},
  volume={abs/2010.12812},
  url={https://api.semanticscholar.org/CorpusID:232320859}
}

@inproceedings{sun-etal-2019-joint,
    title = "Joint Type Inference on Entities and Relations via Graph Convolutional Networks",
    author = "Sun, Changzhi  and
      Gong, Yeyun  and
      Wu, Yuanbin  and
      Gong, Ming  and
      Jiang, Daxin  and
      Lan, Man  and
      Sun, Shiliang  and
      Duan, Nan",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'\i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1131",
    doi = "10.18653/v1/P19-1131",
    pages = "1361--1370",
    abstract = "We develop a new paradigm for the task of joint entity relation extraction. It first identifies entity spans, then performs a joint inference on entity types and relation types. To tackle the joint type inference task, we propose a novel graph convolutional network (GCN) running on an entity-relation bipartite graph. By introducing a binary relation classification task, we are able to utilize the structure of entity-relation bipartite graph in a more efficient and interpretable way. Experiments on ACE05 show that our model outperforms existing joint models in entity performance and is competitive with the state-of-the-art in relation performance.",
}

@inproceedings{fu-etal-2019-graphrel,
    title = "{G}raph{R}el: Modeling Text as Relational Graphs for Joint Entity and Relation Extraction",
    author = "Fu, Tsu-Jui  and
      Li, Peng-Hsuan  and
      Ma, Wei-Yun",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'\i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1136",
    doi = "10.18653/v1/P19-1136",
    pages = "1409--1418",
    abstract = "In this paper, we present GraphRel, an end-to-end relation extraction model which uses graph convolutional networks (GCNs) to jointly learn named entities and relations. In contrast to previous baselines, we consider the interaction between named entities and relations via a 2nd-phase relation-weighted GCN to better extract relations. Linear and dependency structures are both used to extract both sequential and regional features of the text, and a complete word graph is further utilized to extract implicit features among all word pairs of the text. With the graph-based approach, the prediction for overlapping relations is substantially improved over previous sequential approaches. We evaluate GraphRel on two public datasets: NYT and WebNLG. Results show that GraphRel maintains high precision while increasing recall substantially. Also, GraphRel outperforms previous work by 3.2{\%} and 5.8{\%} (F1 score), achieving a new state-of-the-art for relation extraction.",
}

@inproceedings{li-etal-2019-entity,
    title = "Entity-Relation Extraction as Multi-Turn Question Answering",
    author = "Li, Xiaoya  and
      Yin, Fan  and
      Sun, Zijun  and
      Li, Xiayu  and
      Yuan, Arianna  and
      Chai, Duo  and
      Zhou, Mingxin  and
      Li, Jiwei",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'\i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1129",
    doi = "10.18653/v1/P19-1129",
    pages = "1340--1350",
    abstract = "In this paper, we propose a new paradigm for the task of entity-relation extraction. We cast the task as a multi-turn question answering problem, i.e., the extraction of entities and elations is transformed to the task of identifying answer spans from the context. This multi-turn QA formalization comes with several key advantages: firstly, the question query encodes important information for the entity/relation class we want to identify; secondly, QA provides a natural way of jointly modeling entity and relation; and thirdly, it allows us to exploit the well developed machine reading comprehension (MRC) models. Experiments on the ACE and the CoNLL04 corpora demonstrate that the proposed paradigm significantly outperforms previous best models. We are able to obtain the state-of-the-art results on all of the ACE04, ACE05 and CoNLL04 datasets, increasing the SOTA results on the three datasets to 49.6 (+1.2), 60.3 (+0.7) and 69.2 (+1.4), respectively. Additionally, we construct and will release a newly developed dataset RESUME, which requires multi-step reasoning to construct entity dependencies, as opposed to the single-step dependency extraction in the triplet exaction in previous datasets. The proposed multi-turn QA model also achieves the best performance on the RESUME dataset.",
}

@inproceedings{miwa-bansal-2016-end,
    title = "End-to-End Relation Extraction using {LSTM}s on Sequences and Tree Structures",
    author = "Miwa, Makoto  and
      Bansal, Mohit",
    editor = "Erk, Katrin  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1105",
    doi = "10.18653/v1/P16-1105",
    pages = "1105--1116",
}

@inproceedings{bekoulis-etal-2018-adversarial,
    title = "Adversarial training for multi-context joint entity and relation extraction",
    author = "Bekoulis, Giannis  and
      Deleu, Johannes  and
      Demeester, Thomas  and
      Develder, Chris",
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1307",
    doi = "10.18653/v1/D18-1307",
    pages = "2830--2836",
    abstract = "Adversarial training (AT) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data. We show how to use AT for the tasks of entity recognition and relation extraction. In particular, we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations, allows improving the state-of-the-art effectiveness on several datasets in different contexts (i.e., news, biomedical, and real estate data) and for different languages (English and Dutch).",
}

@inproceedings{luan-etal-2019-general,
    title = "A general framework for information extraction using dynamic span graphs",
    author = "Luan, Yi  and
      Wadden, Dave  and
      He, Luheng  and
      Shah, Amy  and
      Ostendorf, Mari  and
      Hajishirzi, Hannaneh",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1308",
    doi = "10.18653/v1/N19-1308",
    pages = "3036--3046",
    abstract = "We introduce a general framework for several information extraction tasks that share span representations using dynamically constructed span graphs. The graphs are dynamically constructed by selecting the most confident entity spans and linking these nodes with confidence-weighted relation types and coreferences. The dynamic span graph allow coreference and relation type confidences to propagate through the graph to iteratively refine the span representations. This is unlike previous multi-task frameworks for information extraction in which the only interaction between tasks is in the shared first-layer LSTM. Our framework significantly outperforms state-of-the-art on multiple information extraction tasks across multiple datasets reflecting different domains. We further observe that the span enumeration approach is good at detecting nested span entities, with significant F1 score improvement on the ACE dataset.",
}

@article{Wadden2019EntityRA,
  title={Entity, Relation, and Event Extraction with Contextualized Span Representations},
  author={David Wadden and Ulme Wennberg and Yi Luan and Hannaneh Hajishirzi},
  journal={ArXiv},
  year={2019},
  volume={abs/1909.03546},
  url={https://api.semanticscholar.org/CorpusID:202539496}
}

@inproceedings{lee-etal-2017-end,
    title = "End-to-end Neural Coreference Resolution",
    author = "Lee, Kenton  and
      He, Luheng  and
      Lewis, Mike  and
      Zettlemoyer, Luke",
    editor = "Palmer, Martha  and
      Hwa, Rebecca  and
      Riedel, Sebastian",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1018",
    doi = "10.18653/v1/D17-1018",
    pages = "188--197",
    abstract = "We introduce the first end-to-end coreference resolution model and show that it significantly outperforms all previous work without using a syntactic parser or hand-engineered mention detector. The key idea is to directly consider all spans in a document as potential mentions and learn distributions over possible antecedents for each. The model computes span embeddings that combine context-dependent boundary representations with a head-finding attention mechanism. It is trained to maximize the marginal likelihood of gold antecedent spans from coreference clusters and is factored to enable aggressive pruning of potential mentions. Experiments demonstrate state-of-the-art performance, with a gain of 1.5 F1 on the OntoNotes benchmark and by 3.1 F1 using a 5-model ensemble, despite the fact that this is the first approach to be successfully trained with no external resources.",
}

@inproceedings{he-etal-2018-jointly,
    title = "Jointly Predicting Predicates and Arguments in Neural Semantic Role Labeling",
    author = "He, Luheng  and
      Lee, Kenton  and
      Levy, Omer  and
      Zettlemoyer, Luke",
    editor = "Gurevych, Iryna  and
      Miyao, Yusuke",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-2058",
    doi = "10.18653/v1/P18-2058",
    pages = "364--369",
    abstract = "Recent BIO-tagging-based neural semantic role labeling models are very high performing, but assume gold predicates as part of the input and cannot incorporate span-level features. We propose an end-to-end approach for jointly predicting all predicates, arguments spans, and the relations between them. The model makes independent decisions about what relationship, if any, holds between every possible word-span pair, and learns contextualized span representations that provide rich, shared input features for each decision. Experiments demonstrate that this approach sets a new state of the art on PropBank SRL without gold predicates.",
}

@inproceedings{lin-etal-2020-joint,
    title = "A Joint Neural Model for Information Extraction with Global Features",
    author = "Lin, Ying  and
      Ji, Heng  and
      Huang, Fei  and
      Wu, Lingfei",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.713",
    doi = "10.18653/v1/2020.acl-main.713",
    pages = "7999--8009",
    abstract = "Most existing joint neural models for Information Extraction (IE) use local task-specific classifiers to predict labels for individual instances (e.g., trigger, relation) regardless of their interactions. For example, a victim of a die event is likely to be a victim of an attack event in the same sentence. In order to capture such cross-subtask and cross-instance inter-dependencies, we propose a joint neural framework, OneIE, that aims to extract the globally optimal IE result as a graph from an input sentence. OneIE performs end-to-end IE in four stages: (1) Encoding a given sentence as contextualized word representations; (2) Identifying entity mentions and event triggers as nodes; (3) Computing label scores for all nodes and their pairwise links using local classifiers; (4) Searching for the globally optimal graph with a beam decoder. At the decoding stage, we incorporate global features to capture the cross-subtask and cross-instance interactions. Experiments show that adding global features improves the performance of our model and achieves new state of-the-art on all subtasks. In addition, as OneIE does not use any language-specific feature, we prove it can be easily applied to new languages or trained in a multilingual manner.",
}

@misc{devlin2019bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{fang-etal-2021-tebner,
    title = "{TEBNER}: Domain Specific Named Entity Recognition with Type Expanded Boundary-aware Network",
    author = "Fang, Zheng  and
      Cao, Yanan  and
      Li, Tai  and
      Jia, Ruipeng  and
      Fang, Fang  and
      Shang, Yanmin  and
      Lu, Yuhai",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.18",
    doi = "10.18653/v1/2021.emnlp-main.18",
    pages = "198--207",
    abstract = "To alleviate label scarcity in Named Entity Recognition (NER) task, distantly supervised NER methods are widely applied to automatically label data and identify entities. Although the human effort is reduced, the generated incomplete and noisy annotations pose new challenges for learning effective neural models. In this paper, we propose a novel dictionary extension method which extracts new entities through the type expanded model. Moreover, we design a multi-granularity boundary-aware network which detects entity boundaries from both local and global perspectives. We conduct experiments on different types of datasets, the results show that our model outperforms previous state-of-the-art distantly supervised systems and even surpasses the supervised models.",
}

@misc{jiang2020complex,
      title={Complex Relation Extraction: Challenges and Opportunities}, 
      author={Haiyun Jiang and Qiaoben Bao and Qiao Cheng and Deqing Yang and Li Wang and Yanghua Xiao},
      year={2020},
      eprint={2012.04821},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{le2018improving,
      title={Improving Entity Linking by Modeling Latent Relations between Mentions}, 
      author={Phong Le and Ivan Titov},
      year={2018},
      eprint={1804.10637},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{tsai-etal-2016-cross,
    title = "Cross-Lingual Named Entity Recognition via Wikification",
    author = "Tsai, Chen-Tse  and
      Mayhew, Stephen  and
      Roth, Dan",
    editor = "Riezler, Stefan  and
      Goldberg, Yoav",
    booktitle = "Proceedings of the 20th {SIGNLL} Conference on Computational Natural Language Learning",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/K16-1022",
    doi = "10.18653/v1/K16-1022",
    pages = "219--228",
}

@INPROCEEDINGS{8520330,
  author={Bhandari, Nitin and Chowdri, Ritika and Singh, Harmeet and Qureshi, Salim Raza},
  booktitle={2017 International Conference on Next Generation Computing and Information Systems (ICNGCIS)}, 
  title={Resolving Ambiguities in Named Entity Recognition Using Machine Learning}, 
  year={2017},
  volume={},
  number={},
  pages={159-163},
  keywords={Information retrieval;Machine learning;Probability;Natural language processing;Hidden Markov models;Text recognition;NER;Natural language processing;Supervised Learning;Na√Øve Bayes;features},
  doi={10.1109/ICNGCIS.2017.24}}

@inproceedings{zhang-etal-2019-ernie,
    title = "{ERNIE}: Enhanced Language Representation with Informative Entities",
    author = "Zhang, Zhengyan  and
      Han, Xu  and
      Liu, Zhiyuan  and
      Jiang, Xin  and
      Sun, Maosong  and
      Liu, Qun",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'\i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1139",
    doi = "10.18653/v1/P19-1139",
    pages = "1441--1451",
    abstract = "Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The code and datasets will be available in the future.",
}

@inproceedings{baldini-soares-etal-2019-matching,
    title = "Matching the Blanks: Distributional Similarity for Relation Learning",
    author = "Baldini Soares, Livio  and
      FitzGerald, Nicholas  and
      Ling, Jeffrey  and
      Kwiatkowski, Tom",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'\i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1279",
    doi = "10.18653/v1/P19-1279",
    pages = "2895--2905",
    abstract = "General purpose relation extractors, which can model arbitrary relations, are a core aspiration in information extraction. Efforts have been made to build general purpose extractors that represent relations with their surface forms, or which jointly embed surface forms with relations from an existing knowledge graph. However, both of these approaches are limited in their ability to generalize. In this paper, we build on extensions of Harris{'} distributional hypothesis to relations, as well as recent advances in learning text representations (specifically, BERT), to build task agnostic relation representations solely from entity-linked text. We show that these representations significantly outperform previous work on exemplar based relation extraction (FewRel) even without using any of that task{'}s training data. We also show that models initialized with our task agnostic representations, and then tuned on supervised relation extraction datasets, significantly outperform the previous methods on SemEval 2010 Task 8, KBP37, and TACRED",
}

@misc{peters2019knowledge,
      title={Knowledge Enhanced Contextual Word Representations}, 
      author={Matthew E. Peters and Mark Neumann and Robert L. Logan IV au2 and Roy Schwartz and Vidur Joshi and Sameer Singh and Noah A. Smith},
      year={2019},
      eprint={1909.04164},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{zhang-etal-2017-position,
    title = "Position-aware Attention and Supervised Data Improve Slot Filling",
    author = "Zhang, Yuhao  and
      Zhong, Victor  and
      Chen, Danqi  and
      Angeli, Gabor  and
      Manning, Christopher D.",
    editor = "Palmer, Martha  and
      Hwa, Rebecca  and
      Riedel, Sebastian",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1004",
    doi = "10.18653/v1/D17-1004",
    pages = "35--45",
    abstract = "Organized relational knowledge in the form of {``}knowledge graphs{''} is important for many applications. However, the ability to populate knowledge bases with facts automatically extracted from documents has improved frustratingly slowly. This paper simultaneously addresses two issues that have held back prior work. We first propose an effective new model, which combines an LSTM sequence model with a form of entity position-aware attention that is better suited to relation extraction. Then we build TACRED, a large (119,474 examples) supervised relation extraction dataset obtained via crowdsourcing and targeted towards TAC KBP relations. The combination of better supervised data and a more appropriate high-capacity model enables much better relation extraction performance. When the model trained on this new dataset replaces the previous relation extraction component of the best TAC KBP 2015 slot filling system, its F1 score increases markedly from 22.2{\%} to 26.7{\%}.",
}