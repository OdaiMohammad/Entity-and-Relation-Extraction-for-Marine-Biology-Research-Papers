% %!TEX root = ../main.tex
\chapter{Related Work}
\label{chp:relatedWork}

With the rise of the Internet, there has been a notable surge in digital text creation across various platforms such as social media, emails, blogs, news articles, publications, and online forums. This vast corpus of unstructured or semi-structured text harbors a wealth of information. Information Extraction (IE) is a pivotal tool in discerning and organizing meaningful insights from these textual sources, transforming them into structured data.

One way to represent information in the text is in the form of entities and relations representing links between entities. Therefore, Named Entity Recognition (NER) and Relation Extraction (RE) emerge as particularly valuable techniques and key components of IE. They enable the extraction of pertinent entities and relationships within the text, facilitating the conversion of raw data into structured repositories of valuable information.

The NER identifies entities from the text, and the RE task can identify relationships between those entities. Furthermore, end-to-end relation extraction aims to identify named entities and extract relations between them in one go. Effectively modeling these two subtasks jointly\cite{Zhong2020AFE}, either by casting them in one structured prediction framework, or performing multi-task learning through shared representations.

Many NLP applications can benefit from relational information derived from natural language\cite{Goyal2018RNE}, including Structured Search, Knowledge Base (KB) population, Information Retrieval, Question-Answering, Language Understanding, Ontology Learning, etc. Therefore these tasks have been studied extensively and many models have been proposed to tackle them.

\section{Datasets}
\subsection{Automatic Content Extraction (ACE)}
Talk about ACE
\subsection{SciERC}
Talk about SciERC
\subsection{Other datasets}
Talk about other datasets


\section{Entity and Relation Extraction Models}

We can recognize two categories of joint models: Structured prediction and Multi-task learning models.

\section{Structured prediction models}
Structured prediction approaches cast the two tasks into one unified framework, although it can be formulated in various ways.\\
Li and Ji\cite{li-ji-2014-incremental} proposed an action-based system that identifies new entities as well as links to previous entities, Zhang et al.\cite{zhang-etal-2017-end};
A novel and impactful methodology for the incremental joint extraction of entity mentions and relations. Their approach diverges from traditional methods by employing a structured perceptron with beam-search, moving away from token-based tagging to a segment-based decoder inspired by semi-Markov chains. This shift allows for the utilization of global features as soft constraints, effectively capturing the interdependencies between entities and relations. Their research, conducted on the Automatic Content Extraction (ACE) corpora, demonstrates significant advancements over existing pipelined approaches. By formulating the problem as one of structured prediction, their model adeptly captures the linguistic and logical nuances inherent in the complex web of textual relationships, thereby addressing the limitations of sequential classification steps that fail to model long-distance and cross-task dependencies. The introduction of novel global features based on soft constraints over the entire output graph structure marks a significant contribution to the field, showcasing the potential for improved accuracy and efficiency in the extraction tasks. Li and Ji's work stands as a pivotal reference in the exploration of joint models and global features for enhancing entity and relation extraction, offering valuable insights and methodologies that could be adapted and extended within the context of marine biology research papers.\\

Wang and Lu\cite{wang-lu-2020-two} adopt a table-filling approach proposed in (Miwa and Sasaki\cite{miwa-sasaki-2014-modeling});
This distinct approach departs from traditional single-encoder methods. Their method introduces two specialized encoders: a table encoder and a sequence encoder, designed to synergize in the representation learning process for Named Entity Recognition (NER) and Relation Extraction (RE). This bifurcation allows each encoder to focus on the unique aspects of its task—capturing task-specific information effectively—while benefiting from the interaction between the two to enhance overall performance. They leverage multi-dimensional recurrent neural networks to better utilize the structural information within the table representation, addressing a common limitation in existing methods that often overlook or underutilize such information. Furthermore, they novelly exploit the pairwise self-attention weights from pre-trained models like BERT to enrich their model's understanding of word-word interactions, a strategy not previously employed for table representations in this context. Their experiments across several standard datasets show significant improvements over existing approaches, particularly highlighting the advantage of dual encoders over traditional single-encoder frameworks. This work not only sets new state-of-the-art performance benchmarks but also opens up new avenues for leveraging the inherent structure in linguistic data for information extraction tasks.\\

Katiyar and Cardie\cite{katiyar-cardie-2017-going} and Zheng et al.\cite{zheng-etal-2017-joint} introduced an approach based on sequence-tagging for the joint extraction of entity mentions and relations, each contributing novel methodologies to the domain of information extraction. Katiyar and Cardie introduce an attention-based recurrent neural network model that leverages long short-term memory (LSTM) networks to extract semantic relations between entity mentions without relying on dependency trees. Their model is distinct for its direct addressing of the relation extraction task by integrating attention mechanisms with LSTMs, enabling the model to focus on relevant parts of the text to better identify relationships between entities, even when they are not adjacent. This approach sidesteps the need for dependency tree information, making it more broadly applicable, especially for languages or domains where dependency parsing might be less accurate or entirely unavailable. Their experiments on the Automatic Content Extraction (ACE) corpora demonstrate significant improvements over previously established feature-based joint models, highlighting the efficacy of their methodology in enhancing the accuracy of both entity and relation extraction tasks.

Zheng et al. propose a different take on sequence tagging by introducing a novel tagging scheme that converts the joint task of entity and relation extraction into a single tagging problem. This simplifies the traditionally complex process of first identifying entities and then classifying relations between them. Their end-to-end model, based on LSTM networks, directly extracts entities and their relations without the need for separate entity recognition and relation classification stages. By treating the problem as a tagging issue, they manage to avoid the error propagation and complexity associated with pipelined and feature-based methods. Their approach not only demonstrates superior performance on a public dataset produced by distant supervision methods but also underscores the potential of tagging-based methods in streamlining the extraction process and improving result accuracy.

These contributions represent significant advancements in the field of information extraction, particularly in the context of NER and RE. They offer insights into the potential of neural network architectures and tagging schemes to simplify and enhance the joint extraction of entities and relations, paving the way for more efficient and accurate extraction methodologies suitable for a wide range of applications.
Sun et al.\cite{sun-etal-2019-joint} and Fu et al.\cite{fu-etal-2019-graphrel} used a graph-based method to predict entity and relation types, offering significant advancements in joint entity and relation extraction tasks. Sun et al. introduced a novel graph convolutional network (GCN) approach that operates on an entity-relation bipartite graph, designed to perform joint inference on entity types and relation types within a unified framework. This method significantly outperformed existing joint models in entity performance while maintaining competitive relation performance on the ACE05 dataset. The key to their approach was the introduction of a binary relation classification task that allowed for more efficient and interpretable use of the entity-relation bipartite graph structure.

Fu et al. presented GraphRel, an end-to-end relation extraction model employing graph convolutional networks (GCNs) to jointly learn named entities and relations. By considering both the interaction between named entities and relations and the implicit features among all word pairs in the text, GraphRel demonstrated substantial improvements in predicting overlapping relations compared to previous sequential approaches. Their graph-based strategy, which utilized both linear and dependency structures, alongside a complete word graph to extract features, resulted in high precision and a significant increase in recall, setting new state-of-the-art benchmarks for relation extraction on public datasets like NYT and WebNLG.

These contributions reflect a deeper understanding of how entities and relations interconnect within text, underscoring the potential of graph-based models to capture complex relationships more effectively than traditional methods. Through the integration of GCNs and strategic graph construction, both approaches highlight the evolving landscape of natural language processing, where the interconnectedness of textual elements is increasingly recognized and leveraged for more nuanced and accurate information extraction.\\

and, Li et al\cite{li-etal-2019-entity} project the task onto a multi-turn question answering problem, transforming the entity and relation extraction process into an innovative QA framework. This paradigm shift offers several advantages: it encodes specific class information for the desired entity or relation through the formulation of questions, naturally incorporates joint modeling of entities and relations, and leverages advanced machine reading comprehension (MRC) models. Their approach not only significantly outperforms existing models on benchmark datasets like ACE and CoNLL04 but also establishes new state-of-the-art results, highlighting its effectiveness in accurately identifying structured information from text. Moreover, Li et al. introduce a complex dataset, RESUME, requiring multi-step reasoning for entity dependency construction, further demonstrating the model's capability in handling intricate entity-relation mappings. This multi-turn QA framework marks a substantial advance in entity-relation extraction, showing promise for more nuanced and accurate information extraction from unstructured data.\\
All of these approaches need to tackle a global optimization problem and perform joint decoding at inference time, using beam search or reinforcement learning.

\section{Multi-task learning models}
This family of models essentially builds two separate models for entity recognition and relation extraction and optimizes them together through parameter sharing.\\
Miwa and Bansal\cite{miwa-bansal-2016-end} propose to use a sequence tagging model for entity prediction and a tree-based LSTM model for relation extraction. The two models share one LSTM layer for contextualized word representations and they find sharing parameters improves performance (slightly) for both models. Their innovative approach captures both word sequence and dependency tree substructure information, integrating bidirectional tree-structured LSTM-RNNs on top of bidirectional sequential LSTM-RNNs. This allows for a single model to jointly represent entities and relations with shared parameters, improving over the state-of-the-art feature-based models on end-to-end relation extraction tasks. By encouraging the detection of entities during training and utilizing entity information in relation extraction through entity pretraining and scheduled sampling, Miwa and Bansal's model demonstrates substantial error reductions in F1-score on both ACE2005 and ACE2004 datasets, setting new benchmarks for the field. Their work underscores the importance of combining linear sequence and tree structure representations for capturing the nuances of entity and relation extraction in text.
The approach of Bekoulis et al.\cite{bekoulis-etal-2018-adversarial} is similar except that they model relation classification as a multi-label head selection problem. Note that these approaches still perform pipelined decoding: entities are first extracted and the relation model is applied on the predicted entities. In their work on adversarial training for multi-context joint entity and relation extraction, Bekoulis et al. extend a baseline joint model that tackles named entity recognition (NER) and relation extraction simultaneously, by introducing adversarial training (AT) as a regularization method. This technique enhances the model's robustness by incorporating small perturbations in the training data, thereby improving the state-of-the-art effectiveness across several datasets and languages. Their model successfully addresses the complexities of extracting multiple relations per entity by modeling relation extraction in a multi-label setting, allowing for a more nuanced understanding of the text. Additionally, their innovative use of AT demonstrates a significant improvement in the joint extraction task's effectiveness, showcasing the potential of adversarial examples in NLP to refine and strengthen model performance.\\
DYGIE and DYGIE++ (Luan et al. \cite{luan-etal-2019-general}; Wadden et al. \cite{Wadden2019EntityRA}), build on recent span-based models for coreference resolution (Lee et al. \cite{lee-etal-2017-end}) and semantic role labeling (He et al. \cite{he-etal-2018-jointly}). The key idea of their approaches is to learn shared span representations between the two tasks and update span representations through dynamic graph propagation layers.  DYGIE++ extends upon these concepts by incorporating event extraction into its multi-task framework, utilizing both local (within-sentence) and global (cross-sentence) context to enumerate, refine, and score text spans. The system dynamically constructs graphs of spans, with edges representing task-specific relations, allowing for efficient global context modeling. This is achieved by refining initial contextualized embeddings, such as those from BERT, with task-specific message updates propagated across the span graph.

The DYGIE++ framework demonstrates its effectiveness by achieving state-of-the-art results across several information extraction tasks and datasets, showcasing its capability to handle complex interdependencies among entities, relations, and events. The integration of BERT encodings enables the model to capture significant contextual relationships, including those extending beyond single sentences. Additionally, dynamic span graph updates further enhance the model's ability to incorporate cross-sentence dependencies, which is particularly beneficial for tasks in specialized domains. For example, leveraging predicted coreference links through graph propagation can help disambiguate challenging entity mentions by providing additional contextual clues.

A comprehensive evaluation of the DYGIE++ framework across different datasets reveals that its general span-based approach produces significant improvements in entity recognition, relation extraction, and event extraction tasks. The framework benefits from both types of contextualization methods—BERT encodings for capturing immediate and adjacent-sentence context, and message passing updates for modeling long-range cross-sentence dependencies. These findings underscore the importance of effectively integrating both local and global contextual information in a unified architecture to enhance performance on a range of information extraction tasks, making DYGIE++ a powerful tool for advancing research in this area.
A more recent work Lin et al.\cite{lin-etal-2020-joint} further extends DYGIE++ by incorporating global features based on cross-substask and cross-instance constraints.  They propose a joint neural framework named ONEIE, which aims to extract the globally optimal Information Extraction (IE) result as a graph from an input sentence. This framework performs IE in four stages: encoding the given sentence as contextualized word representations; identifying entity mentions and event triggers as nodes; computing label scores for all nodes and their pairwise links using local classifiers; and finally, searching for the globally optimal graph with a beam decoder. At the decoding stage, they introduce global features to capture the intricate cross-subtask and cross-instance interactions. Their experimental results demonstrate that adding these global features significantly improves the performance of their model, achieving new state-of-the-art results on all subtasks. Unlike previous models that use separate local task-specific classifiers in their final layer without explicitly modeling the dependencies among tasks and instances, ONEIE extracts a unified graph representation of the input sentence, effectively capturing and leveraging the interdependencies among different IE components. This advancement underscores the importance of considering the holistic context of information in IE tasks, marking a significant step forward in the development of more integrated and contextually aware IE systems.
Zhong et al. \cite{Zhong2020AFE} propose a similar approach. But it is much simpler and performs better, challenging the longstanding belief in the superiority of complex joint models for entity and relation extraction tasks. Through their research, they illuminate the effectiveness of a straightforward pipelined approach that employs two independent encoders for entity recognition and relation extraction, both built upon deep pre-trained language models. Their method deviates from the common practice of intricate joint modeling, advocating instead for the simplicity and directness in treating the tasks sequentially. This simplicity, coupled with meticulous analyses on standard benchmarks like ACE04, ACE05, and SciERC, not only set new state-of-the-art performances with absolute improvements in relation F1 scores but also demonstrate the critical importance of learning distinct contextual representations for entities and relations. Furthermore, their investigation into incorporating entity information early in the relation model underscores the potential of a more focused approach in enhancing performance.

Their work significantly contributes to the discourse on the efficiency of information extraction models, showing that a model's complexity does not necessarily equate to its effectiveness. By simplifying the process into two distinct phases and ensuring each phase is optimized for its specific task, they reveal an often-overlooked aspect of model design: the power of specialization and focused optimization. Their findings suggest that the interactions between entities and relations, previously believed to be best captured jointly, can be effectively understood through a well-structured sequential approach. This revelation opens new avenues for future research in information extraction, particularly in exploring how different tasks within this domain can be optimized individually for better overall performance.

Moreover, the paper's exploration into the utility of pre-trained language models as a foundation for both encoders brings to light the substantial impact of these models in extracting meaningful insights from text. By leveraging such powerful models, Zhong et al. manage to not only streamline the extraction process but also ensure that their approach remains both flexible and robust across various datasets. This adaptability, combined with the method's simplicity, marks a significant step forward in information extraction research. It prompts a reevaluation of current methodologies and suggests that the field might benefit from a shift towards simpler, more focused models that capitalize on the advancements in language modeling and representation learning.

\section{Performance comparison}
Compare the performance of the models mentioned above in a table comparing P, R, and F1 on different datasets.



